_target_: zatom.models.zatom2.Zatom

architecture:
  max_num_elements: ${globals.max_num_elements}
  add_mask_atom_type: true

augmentations:
  frac_coords: true
  pos: true
  scale: 1.0
  ref_scale: 1.0 # NOTE: currently applied (multiplicitively) to noisy atom `pos` after normalizing by `scale`
  multiplicity: 8 # number of copies per GPU

sampling:
  cfg_scale: 2.0 # classifier-free guidance scale
  num_samples: 1000
  batch_size: 100
  steps: 100 # number of ODE steps
  data_dir: ${paths.data_dir}
  visualize: true
  save_dir: ${paths.viz_dir}
  removeHs: false # only retain heavy atoms (for molecules)
  mask_token_index: ${model.architecture.max_num_elements}

conditioning:
  dataset_idx: true # always true
  spacegroup: false

datasets: ${data.datamodule.datasets}

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 5e-4
  weight_decay: 0.0
  betas: [0.9, 0.995]

scheduler:
  _target_: ${resolve_variable:torch.optim.lr_scheduler.LambdaLR}
  _partial_: true
  lr_lambda: ${resolve_lr_scheduler:constant_schedule_with_warmup,10}

scheduler_frequency: 1
# scheduler_frequency: ${trainer.check_val_every_n_epoch}

# compile model for faster training with pytorch 2.0
compile: true

# using WandB, log model gradients every N steps
log_grads_every_n_steps: 1000
