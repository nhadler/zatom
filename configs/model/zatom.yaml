_target_: zatom.models.zatom.Zatom

architecture:
  max_num_elements: ${globals.max_num_elements}
  force_loss_weight: 5.0

augmentations:
  frac_coords: true
  pos: true
  scale: 0.5 # NOTE: currently applied (multiplicatively) to target atom `pos` before adding noise
  ref_scale: 1.0 # NOTE: currently applied (multiplicatively) to target atom `pos` after normalizing by `scale`
  multiplicity: 8 # number of copies per GPU

sampling:
  cfg_scale: 0.0 # classifier-free guidance scale, 0.0 = no guidance
  num_samples: 1000
  batch_size: 100
  steps: 100 # number of ODE steps
  data_dir: ${paths.data_dir}
  visualize: true
  save_dir: ${paths.viz_dir}
  removeHs: false # only retain heavy atoms (for molecules)
  mask_token_index: ${model.architecture.max_num_elements}

conditioning:
  dataset_idx: true # always true
  spacegroup: false

datasets: ${data.datamodule.datasets}

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: ${resolve_lr:5e-4,${data.datamodule.batch_size.base_world_size},${multiply:${trainer.devices},${trainer.num_nodes}},${model.architecture.batch_size_scale_factor}}
  weight_decay: 0.0
  betas: [0.9, 0.995]

scheduler:
  # _target_: ${resolve_variable:torch.optim.lr_scheduler.LambdaLR}
  # _partial_: true
  # lr_lambda: ${resolve_lr_scheduler:constant_schedule_with_warmup,10}

scheduler_frequency: 1
# scheduler_frequency: ${trainer.check_val_every_n_epoch}

# compile model for faster training with pytorch 2.0
compile: true

# using WandB, log model gradients every N steps
log_grads_every_n_steps: 1000

# Name of the task being performed (e.g., `train_fm`, `finetune_fm`, `eval_fm`)
task_name: ${task_name}
