_target_: zatom.models.architectures.mft.MFT

# Modules
multimodal_model:
  _target_: zatom.models.architectures.dit.dit.MultimodalDiT
  _partial_: True

time_embedder:
  _target_: zatom.models.architectures.dit.layers.TimestepEmbedder
  hidden_size: ${model.architecture.hidden_size}

dataset_embedder:
  _target_: zatom.models.architectures.dit.layers.LabelEmbedder
  num_classes: ${globals.num_dataset_classes}
  hidden_size: ${model.architecture.hidden_size}
  dropout_prob: 0.1

spacegroup_embedder:
  _target_: zatom.models.architectures.dit.layers.LabelEmbedder
  num_classes: ${globals.num_spacegroup_classes}
  hidden_size: ${model.architecture.hidden_size}
  dropout_prob: 0.1

token_pos_embedder:
  _target_: zatom.models.architectures.dit.pos_embed.AbsolutePositionEncoding
  in_dim: 1
  embed_dim: ${model.architecture.hidden_size}
  include_input: True

atom_pos_embedder:
  _target_: zatom.models.architectures.dit.pos_embed.FourierPositionEncoding
  in_dim: ${globals.spatial_dim}
  include_input: True
  min_freq_log2: 0
  max_freq_log2: 12
  num_freqs: 128
  log_sampling: True

atom_encoder_transformer:
  _target_: zatom.models.architectures.dit.blocks.HomogenTrunk
  depth: 2
  block:
    _target_: zatom.models.architectures.dit.blocks.DiTBlock
    _partial_: True # NOTE: this is set since, in the for-loop, we create a new module
    hidden_size: ${model.architecture.atom_hidden_size_enc}
    mlp_ratio: 4.0
    use_swiglu: True # SwiGLU FFN
    self_attention_layer:
      _target_: zatom.models.architectures.dit.layers.EfficientSelfAttentionLayer
      _partial_: True
      hidden_size: ${model.architecture.atom_hidden_size_enc}
      num_heads: ${model.architecture.atom_num_heads}
      qk_norm: True
      jvp_attn: ${model.architecture.jvp_attn}
      pos_embedder:
        _target_: zatom.models.architectures.dit.pos_embed.AxialRotaryPositionEncoding
        in_dim: 4
        embed_dim: ${model.architecture.atom_hidden_size_enc}
        num_heads: ${model.architecture.atom_num_heads}
        base: 100.0

trunk:
  _target_: zatom.models.architectures.dit.blocks.HomogenTrunk
  depth: 24
  repr_layer: 11 # NOTE: by default set to half of depth (zero-indexed)
  block:
    _target_: zatom.models.architectures.dit.blocks.DiTBlock
    _partial_: True # NOTE: this is set since, in the for-loop, we create a new module
    hidden_size: ${model.architecture.hidden_size}
    mlp_ratio: 4.0
    use_swiglu: True # SwiGLU FFN
    self_attention_layer:
      _target_: zatom.models.architectures.dit.layers.EfficientSelfAttentionLayer
      _partial_: True
      hidden_size: ${model.architecture.hidden_size}
      num_heads: ${model.architecture.token_num_heads}
      qk_norm: True
      jvp_attn: ${model.architecture.jvp_attn}
      pos_embedder:
        _target_: zatom.models.architectures.dit.pos_embed.AxialRotaryPositionEncoding
        in_dim: 1
        embed_dim: ${model.architecture.hidden_size}
        num_heads: ${model.architecture.token_num_heads}
        base: 100.0

atom_decoder_transformer:
  _target_: zatom.models.architectures.dit.blocks.HomogenTrunk
  depth: 2
  block:
    _target_: zatom.models.architectures.dit.blocks.DiTBlock
    _partial_: True # NOTE: this is set since, in the for-loop, we create a new module
    hidden_size: ${model.architecture.atom_hidden_size_dec}
    mlp_ratio: 4.0
    use_swiglu: True # SwiGLU FFN
    self_attention_layer:
      _target_: zatom.models.architectures.dit.layers.EfficientSelfAttentionLayer
      _partial_: True
      hidden_size: ${model.architecture.atom_hidden_size_dec}
      num_heads: ${model.architecture.atom_num_heads}
      qk_norm: True
      jvp_attn: ${model.architecture.jvp_attn}
      pos_embedder:
        _target_: zatom.models.architectures.dit.pos_embed.AxialRotaryPositionEncoding
        in_dim: 4
        embed_dim: ${model.architecture.atom_hidden_size_dec}
        num_heads: ${model.architecture.atom_num_heads}
        base: 100.0

# Interpolants for each modality
atom_types_interpolant:
  _target_: zatom.flow.interpolants.DiscreteInterpolant
  key: atom_types
  loss_weight: 0.1
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

pos_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: pos
  loss_weight: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: true
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.9
  white_noise_sampling_scale: 0.01
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

frac_coords_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: frac_coords
  loss_weight: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: false
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.9
  white_noise_sampling_scale: 0.01
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

lengths_scaled_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: lengths_scaled
  loss_weight: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: false
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.9
  white_noise_sampling_scale: 0.01
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

angles_radians_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: angles_radians
  loss_weight: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: false
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.9
  white_noise_sampling_scale: 0.01
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

# General architecture parameters
hidden_size: 1024
token_num_heads: 16
atom_num_heads: 4
atom_hidden_size_enc: 256
atom_hidden_size_dec: 256
atom_n_queries_enc: null # NOTE: if null, user-provided `mask` will be used instead
atom_n_keys_enc: null # NOTE: if null, user-provided `mask` will be used instead
atom_n_queries_dec: null # NOTE: if null, user-provided `mask` will be used instead
atom_n_keys_dec: null # NOTE: if null, user-provided `mask` will be used instead
max_num_elements: ${globals.max_num_elements}
batch_size_scale_factor: 0.5 # 2, 1, 0.5 -> 80M, 180M, 500M model sizes
use_length_condition: true
condition_on_input: false
test_so3_equivariance: false
jvp_attn: false
interdist_loss: null # NOTE: must be one of (`l1`, `l2`, null)
time_distribution: beta # note: must be one of (`uniform`, `beta`, `histogram`)
time_alpha_factor: 1.8
