_target_: zatom.models.architectures.tft.TFT # with TransformerModulePlatonic

# Modules
multimodal_model:
  _target_: zatom.models.architectures.platoformer.transformer_module.TransformerModulePlatonic
  _partial_: True
  solid_name: ${model.architecture.solid_name}
  spatial_dim: ${globals.spatial_dim}
  num_atom_types: ${globals.max_num_elements}
  num_properties: ${globals.num_global_properties}
  context_length: ${globals.max_num_atoms}
  use_sequence_sin_ape: true
  concat_combine_input: false
  is_conservative: false
  sum_pool_energy: false
  separate_energy_force_transformers: true
  mask_material_coords: true
  normalize_per_g: ${model.architecture.normalize_per_g}
  custom_weight_init: null

  coords_embed:
    _target_: zatom.models.architectures.platoformer.positional_encoder.PlatonicLinearAPE
    c_embed: ${model.architecture.hidden_size}
    solid_name: ${model.architecture.solid_name}
    spatial_dims: ${globals.spatial_dim}

  transformer_factory:
    _target_: zatom.models.architectures.platoformer.transformer.ModernTransformerPlatonic
    _partial_: true
    c_qk: ${int_divide:${model.architecture.hidden_size},${model.architecture.token_num_heads}} # per group element and head
    c_val: ${int_divide:${model.architecture.hidden_size},${model.architecture.token_num_heads}} # per group element and head
    n_heads: ${model.architecture.token_num_heads}
    ### Platonic attention specific args
    solid_name: ${model.architecture.solid_name}
    freq_sigma_platonic: null
    freq_init_platonic: "random"
    learned_freqs_platonic: true
    bias: false
    mean_aggregation: false
    linear_attention: false
    use_key: true
    ### Modern attention specific args
    context_length: ${globals.max_num_atoms}
    sequence_rope_base: ${globals.rope_base}
    qk_layernorm: true
    qk_norm_per_g: ${model.architecture.normalize_per_g}
    attn_backend: ${model.architecture.attn_backend}
    ### Normalization args
    normalize_per_g: ${model.architecture.normalize_per_g}
    norm_elementwise_affine: true

  cross_attn_factory:
    _target_: zatom.models.architectures.platoformer.transformer.ModernTransformerDecoderBlockPlatonic
    _partial_: true
    c_model: ${model.architecture.hidden_size}
    c_qk: ${int_divide:${model.architecture.hidden_size},${model.architecture.token_num_heads}} # per group element and head
    c_val: ${int_divide:${model.architecture.hidden_size},${model.architecture.token_num_heads}} # per group element and head
    n_heads: ${model.architecture.token_num_heads}
    ### Platonic attention specific args
    solid_name: ${model.architecture.solid_name}
    freq_sigma_platonic: ${..transformer_factory.freq_sigma_platonic}
    freq_init_platonic: ${..transformer_factory.freq_init_platonic}
    learned_freqs_platonic: ${..transformer_factory.learned_freqs_platonic}
    bias: ${..transformer_factory.bias}
    mean_aggregation: ${..transformer_factory.mean_aggregation}
    linear_attention: ${..transformer_factory.linear_attention}
    use_key: ${..transformer_factory.use_key}
    ### Modern attention specific args
    context_length: ${globals.max_num_atoms}
    sequence_rope_base: ${globals.rope_base}
    use_sequence_rope_cross: true
    qk_layernorm: ${..transformer_factory.qk_layernorm}
    qk_norm_per_g: ${..transformer_factory.qk_norm_per_g}
    attn_backend: ${..transformer_factory.attn_backend}
    ### Normalization args
    normalize_per_g: ${model.architecture.normalize_per_g}
    norm_elementwise_affine: ${..transformer_factory.norm_elementwise_affine}

dataset_embedder:
  _target_: zatom.models.architectures.transformer.common.LabelEmbedder
  num_classes: ${globals.num_dataset_classes}
  hidden_size: ${model.architecture.hidden_size}
  dropout_prob: 0.1

spacegroup_embedder:
  _target_: zatom.models.architectures.transformer.common.LabelEmbedder
  num_classes: ${globals.num_spacegroup_classes}
  hidden_size: ${model.architecture.hidden_size}
  dropout_prob: 0.1

# Interpolants for each modality
atom_types_interpolant:
  _target_: zatom.flow.interpolants.DiscreteInterpolant
  key: atom_types
  loss_weight: 0.1
  path_t_min: 0.0
  path_t_max: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  temperature: 1.0
  top_k: null
  top_p: null
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

pos_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: pos
  loss_weight: 1.0
  path_t_min: 0.0
  path_t_max: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: true
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.95
  white_noise_sampling_scale: 50.0
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

frac_coords_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: frac_coords
  loss_weight: 1.0
  path_t_min: 0.0
  path_t_max: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: false
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.9
  white_noise_sampling_scale: 0.01
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

lengths_scaled_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: lengths_scaled
  loss_weight: 1.0
  path_t_min: 0.0
  path_t_max: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: false
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.9
  white_noise_sampling_scale: 0.01
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

angles_radians_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: angles_radians
  loss_weight: 1.0
  path_t_min: 0.0
  path_t_max: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: false
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.9
  white_noise_sampling_scale: 0.01
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

### General architecture parameters

# Different settings applied in sub-configurations:
# Tetrahedral group (G=12):
#  _______________________________________________________________________________________________________
#  __ch__|__params__|__heuristic_target____|__config_name______________|__memory________|__batch_size_fac_
#    40  |    6.9M  |   6.7M =  80/12      |  tft_ 80M_tet12_divG      |                |
#    60  |   15.5M  |  13.3M = 160/12      |  tft_160M_tet12_divG      |                |
#    70  |   20.9M  |                      |                           |                |
#    74  |   23.3M  |  23.0M =  80/12**.5  |  tft_ 80M_tet12_divSqrtG  |                |  0.5
#    80  |   27.5M  |  25.0M = 300/12      |  tft_300M_tet12_divG      |                |
#   100  |   46.9M  |  46.2M = 160/12**.5  |  tft_160M_tet12_divSqrtG  |                |  0.25
#   140  |   84.1M  |  86.6M = 300/12**.5  |  tft_300M_tet12_divSqrtG  |                |  0.125

# Octahedral group (G=24):
#  _______________________________________________________________________________________________________
#  __ch__|__params__|__heuristic_target____|__config_name______________|__memory________|__batch_size_fac_
#    20  |    3.4M  |   3.3M =  80/24      |  tft_ 80M_oct24_divG      |                |
#    30  |    7.6M  |   6.7M = 160/24      |  tft_160M_oct24_divG      |                |
#    40  |   13.7M  |  12.5M = 300/24      |  tft_300M_oct24_divG      |                |
#    46  |   17.9M  |                      |                           |                |
#    44  |   16.6M  |  16.3M =  80/24**.5  |  tft_ 80M_oct24_divSqrtG  |  50.9Gb (qm9)  |  0.5
#    50  |   21.2M  |                      |                           |                |
#    60  |   30.9M  |  32.7M = 160/24**.5  |  tft_160M_oct24_divSqrtG  |                |
#    70  |   41.7M  |                      |                           |                |
#    80  |   54.9M  |                      |                           |                |
#    84  |   60.5M  |  61.2M = 300/24**.5  |  tft_300M_oct24_divSqrtG  |                |
#    86  |   63.0M  |                      |                           |                |

# TFP specific
solid_name: ??? # explicitly set in sub-configs
batch_size_scale_factor: ??? # explicitly set in sub-configs
hidden_size: ??? # explicitly set in sub-configs
aux_hidden_size: ??? # explicitly set in sub-configs
token_num_heads: 4 # <<<<<<<<<<<<<<<<<<<
normalize_per_g: true

# Same as in TFT
num_layers: 16
num_aux_layers: 0 # NOTE: by default, auxiliary task layers are disabled
num_aux_mlip_layers: ${model.architecture.num_aux_layers} # NOTE: by default, auxiliary energy and force prediction task layers are disabled
aux_mlip_hidden_size: ${model.architecture.aux_hidden_size} # (fixed) hidden size for auxiliary energy and force prediction tasks
aux_layer: 15 # NOTE: by default set to the last layer (zero-indexed)
interdist_loss: null # NOTE: must be one of (`l1`, `l2`, null)
time_distribution: beta # NOTE: must be one of (`uniform`, `beta`, `histogram`)
attn_backend: SDPA # NOTE: must be one of (`SDPA`, `JVP_ATTN`, `MANUAL`)
force_loss_choice: mse # NOTE: must be one of (`mse`, `mae`, `huber`)
time_alpha_factor: 1.8
test_so3_equivariance: false
