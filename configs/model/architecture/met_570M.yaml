_target_: zatom.models.architectures.mft.MFT

encoder: ${model.architecture.encoder}

d_x: ${model.architecture.encoder.d_model}
d_model: 1024 # 384, 768, 1024
num_layers: 24 # 12, 12, 24
nhead: 16 # 6, 12, 16
num_datasets: 2
max_num_elements: 100
batch_size_scale_factor: 0.375 # 2, 1, 0.375
context_length: 2048
rope_base: 10000
mlp_ratio: 4.0
proj_drop: 0.1
attn_drop: 0.0
class_dropout_prob: 0.1
atom_types_loss_weight: 1.0
pos_loss_weight: 10.0
frac_coords_loss_weight: 10.0
lengths_scaled_loss_weight: 1.0
angles_radians_loss_weight: 10.0
grad_decay_a: 0.8
grad_decay_b: 0.8
grad_mul: 1.0
early_stopping_grad_norm: null
qkv_bias: false
qk_norm: true
scale_attn_norm: false
proj_bias: false
flex_attn: false
fused_attn: true
jvp_attn: false
weighted_rigid_align_pos: true
weighted_rigid_align_frac_coords: false
continuous_x_1_prediction: false
unified_modal_time: true
remove_t_conditioning: true
enable_eqm_mode: true
add_mask_atom_type: true
norm_layer: ${resolve_variable:zatom.models.architectures.encoders.custom_transformer.LayerNorm}
grad_decay_method: linear_decay # NOTE: must be one of (`linear_decay`, `truncated_decay`, `piecewise_decay`)
