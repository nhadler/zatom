_target_: zatom.models.architectures.mft.MFT

# Modules
multimodal_model:
  _target_: zatom.models.architectures.dit.dip.MultimodalDiP
  _partial_: True

time_embedder:
  _target_: zatom.models.architectures.dit.layers.TimestepEmbedder
  hidden_size: ${model.architecture.hidden_size}

dataset_embedder:
  _target_: zatom.models.architectures.dit.layers.LabelEmbedder
  num_classes: ${globals.num_dataset_classes}
  hidden_size: ${model.architecture.hidden_size}
  dropout_prob: 0.1

spacegroup_embedder:
  _target_: zatom.models.architectures.dit.layers.LabelEmbedder
  num_classes: ${globals.num_spacegroup_classes}
  hidden_size: ${model.architecture.hidden_size}
  dropout_prob: 0.1

token_pos_embedder:
  _target_: zatom.models.architectures.dit.pos_embed.AbsolutePositionEncoding
  in_dim: 1
  embed_dim: ${model.architecture.hidden_size}
  include_input: True

atom_pos_embedder:
  _target_: platonic_transformers.models.platoformer.ape.PlatonicAPE
  embed_dim: ${model.architecture.hidden_size}
  solid_name: ${model.architecture.solid_name}
  freq_sigma: 0.5
  spatial_dims: ${globals.spatial_dim}
  learned_freqs: True

atom_encoder_transformer:
  _target_: zatom.models.architectures.dit.blocks.HomogenTrunk
  depth: 2
  block:
    _target_: zatom.models.architectures.dit.blocks.DiPBlock
    _partial_: True # NOTE: this is set since, in the for-loop, we create a new module
    hidden_size: ${model.architecture.atom_hidden_size_enc}
    solid: ${model.architecture.solid_name}
    mlp_ratio: 4.0
    use_swiglu: True # SwiGLU FFN
    self_attention_layer:
      _target_: zatom.models.architectures.dit.layers.PlatonicEfficientSelfAttentionLayer
      _partial_: True
      hidden_size: ${model.architecture.atom_hidden_size_enc}
      solid: ${model.architecture.solid_name}
      num_heads: ${model.architecture.atom_num_heads}
      qk_norm: True
      jvp_attn: ${model.architecture.jvp_attn}
      pos_embedder:
        _target_: platonic_transformers.models.platoformer.rope.PlatonicRoPE
        _partial_: True # NOTE: this is set to calculate `num_heads`/`head_dim` correctly during module initialization
        embed_dim: ${model.architecture.atom_hidden_size_enc}
        solid_name: ${model.architecture.solid_name}
        spatial_dims: ${globals.spatial_dim}
        freq_sigma: 1.0
        learned_freqs: True
        freq_init: random # NOTE: must be one of (`random`, `spiral`)

trunk:
  _target_: zatom.models.architectures.dit.blocks.HomogenTrunk
  depth: 16
  block:
    _target_: zatom.models.architectures.dit.blocks.DiPBlock
    _partial_: True # NOTE: this is set since, in the for-loop, we create a new module
    hidden_size: ${model.architecture.hidden_size}
    solid: ${model.architecture.solid_name}
    mlp_ratio: 4.0
    use_swiglu: True # SwiGLU FFN
    self_attention_layer:
      _target_: zatom.models.architectures.dit.layers.PlatonicEfficientSelfAttentionLayer
      _partial_: True
      hidden_size: ${model.architecture.hidden_size}
      solid: ${model.architecture.solid_name}
      num_heads: ${model.architecture.token_num_heads}
      qk_norm: True
      jvp_attn: ${model.architecture.jvp_attn}
      pos_embedder:
        _target_: platonic_transformers.models.platoformer.rope.PlatonicRoPE
        _partial_: True # NOTE: this is set to calculate `num_heads`/`head_dim` correctly during module initialization
        embed_dim: ${model.architecture.hidden_size}
        solid_name: ${model.architecture.solid_name}
        spatial_dims: ${globals.spatial_dim}
        freq_sigma: 1.0
        learned_freqs: True
        freq_init: random # NOTE: must be one of (`random`, `spiral`)

atom_decoder_transformer:
  _target_: zatom.models.architectures.dit.blocks.HomogenTrunk
  depth: 2
  block:
    _target_: zatom.models.architectures.dit.blocks.DiPBlock
    _partial_: True # NOTE: this is set since, in the for-loop, we create a new module
    hidden_size: ${model.architecture.atom_hidden_size_dec}
    solid: ${model.architecture.solid_name}
    mlp_ratio: 4.0
    use_swiglu: True # SwiGLU FFN
    self_attention_layer:
      _target_: zatom.models.architectures.dit.layers.PlatonicEfficientSelfAttentionLayer
      _partial_: True
      hidden_size: ${model.architecture.atom_hidden_size_dec}
      solid: ${model.architecture.solid_name}
      num_heads: ${model.architecture.atom_num_heads}
      qk_norm: True
      jvp_attn: ${model.architecture.jvp_attn}
      pos_embedder:
        _target_: platonic_transformers.models.platoformer.rope.PlatonicRoPE
        _partial_: True # NOTE: this is set to calculate `num_heads`/`head_dim` correctly during module initialization
        embed_dim: ${model.architecture.atom_hidden_size_dec}
        solid_name: ${model.architecture.solid_name}
        spatial_dims: ${globals.spatial_dim}
        freq_sigma: 1.0
        learned_freqs: True
        freq_init: random # NOTE: must be one of (`random`, `spiral`)

# Interpolants for each modality
atom_types_interpolant:
  _target_: zatom.flow.interpolants.DiscreteInterpolant
  key: atom_types
  loss_weight: 0.1
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

pos_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: pos
  loss_weight: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: true
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.9
  white_noise_sampling_scale: 0.01
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

frac_coords_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: frac_coords
  loss_weight: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: false
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.9
  white_noise_sampling_scale: 0.01
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

lengths_scaled_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: lengths_scaled
  loss_weight: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: false
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.9
  white_noise_sampling_scale: 0.01
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

angles_radians_interpolant:
  _target_: zatom.flow.interpolants.SDEMetricInterpolant
  key: angles_radians
  loss_weight: 1.0
  sample_schedule: log # NOTE: must be one of (`linear`, `power`, `log`)
  centered: false
  scale_noise_by_log_num_tokens: false
  noise_scale: 1.0
  langevin_sampling_schedule:
    _target_: zatom.sampling.noise_schedule.SampleNoiseSchedule
    cutoff: 0.9
  white_noise_sampling_scale: 0.01
  time_factor:
    _target_: zatom.flow.time_factor.InverseTimeFactor
    max_value: 100.0
    min_value: 0.05
    zero_before: 0.0
    eps: 1e-6

# General architecture parameters
hidden_size: 3072
token_num_heads: 384
atom_num_heads: 96
atom_hidden_size_enc: 768
atom_hidden_size_dec: 768
atom_n_queries_enc: null # NOTE: if null, user-provided `mask` will be used instead
atom_n_keys_enc: null # NOTE: if null, user-provided `mask` will be used instead
atom_n_queries_dec: null # NOTE: if null, user-provided `mask` will be used instead
atom_n_keys_dec: null # NOTE: if null, user-provided `mask` will be used instead
max_num_elements: ${globals.max_num_elements}
batch_size_scale_factor: 1 # 2, 1, 0.5 -> 80M, 180M, 500M model sizes
use_length_condition: true
condition_on_input: false
test_so3_equivariance: false
jvp_attn: false
interdist_loss: null # NOTE: must be one of (`l1`, `l2`, null)
time_distribution: beta # note: must be one of (`uniform`, `beta`, `histogram`)
time_alpha_factor: 1.8

# Platonic Transformer parameters
solid_name: octahedron # NOTE: must be one of (`trivial`, `tetrahedron`, `octahedron`, `icosahedron`, `octahedron_reflections`)
