_target_: zatom.models.architectures.encoders.transformer.TransformerEncoder

d_model: ${model.architecture.d_model}
nhead: ${model.architecture.nhead}
num_layers: 8
context_length: 2048
rope_base: 10000
mlp_ratio: 4.0
proj_drop: 0.1
attn_drop: 0.0
qkv_bias: false
qk_norm: true
scale_attn_norm: false
scale_mlp_norm: false
proj_bias: false
flex_attn: false
fused_attn: true
jvp_attn: false
checkpoint_activations: false
act_layer: ${resolve_variable:torch.nn.GELU}
norm_layer: ${resolve_variable:zatom.models.architectures.encoders.custom_transformer.LayerNorm}
mlp_layer: ${resolve_variable:zatom.models.architectures.encoders.custom_transformer.Mlp}
