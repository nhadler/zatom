# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.LearningRateMonitor.html

learning_rate_monitor:
  _target_: lightning.pytorch.callbacks.LearningRateMonitor
  logging_interval: epoch # set to `epoch` or `step` to log learning rate of all optimizers at the same interval, or set to `null` to log at individual interval according to the interval key of each scheduler
  log_momentum: true # whether to also log the momentum values of the optimizer, if the optimizer has the `momentum` or `betas` attribute
  log_weight_decay: true # whether to also log the weight decay values of the optimizer, if the optimizer has the `weight_decay` attribute
