_target_: lightning.pytorch.trainer.Trainer

default_root_dir: ${paths.output_dir}

min_epochs: 1 # prevents early stopping
max_epochs: 10000

max_time: "03:00:00:00" # max training time in D:H:M:S format

accelerator: cpu
devices: 1

# mixed precision for extra speed-up
precision: 32-true

# check validation set every N training epochs
val_check_interval: 1.0
check_val_every_n_epoch: 20

# # check validation set every N training batches
# val_check_interval: 1000
# check_val_every_n_epoch: null

# set True to to ensure deterministic results
# makes training slower but gives more reproducibility than just setting seeds
deterministic: False

# if `gradient_clip_val` is not `0.0` or `null`, gradients will be norm-clipped by an upper bound of `gradient_clip_val` during training
gradient_clip_algorithm: norm
gradient_clip_val: 0.5

# accumulate gradients of `accumulate_grad_batches` batches (e.g., by this amount for each rank) before doing a backward pass
# NOTE: here, we would have to accumulate gradients independently for each rank, per https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#accumulate-gradients
accumulate_grad_batches: ${resolve_grad_accum_steps:${data.datamodule.batch_size.base_accumulate_grad_batches},${model.architecture.batch_size_scale_factor}}

# if `num_sanity_val_steps` is > 0, then specifically that many validation steps will be run during the first call to `trainer.fit`
num_sanity_val_steps: 0

# log every N training/validation/test steps
log_every_n_steps: 100

# if `overfit_batches` is > 0, then only the first `overfit_batches` batches of the dataset splits will be used for training,
# which may be useful for debugging and overfitting experiments
overfit_batches: 0
# limit_train_batches: 0.01 # also useful for debugging

# configure inference mode (i.e., `torch.no_grad()` context)
inference_mode: false # NOTE: this must be disabled to calculate gradients for JVP-enabled steps
