_target_: zatom.models.ebm_module.EBMLitModule

ecoder: ${ecoder}

interpolant:
  _target_: zatom.models.interpolants.flow_matching.FlowMatchingInterpolant
  disc_feats: [atom_types]
  cont_feats: [pos, frac_coords, lengths_scaled, angles_radians]
  mask_token_index: ${ecoder.max_num_elements}
  min_t: 0
  max_t: 0
  corrupt: True
  disc_interpolant_type: masking # NOTE: must be one of (`uniform`, `masking`)

augmentations:
  frac_coords: false
  pos: true

sampling:
  cfg_scale: 2.0 # classifier-free guidance scale (only used if `ecoder=mft`)
  num_samples: 1000
  batch_size: 100
  steps: 100 # number of ODE steps (only used if `ecoder=mft`)
  atom_types_temperature: 1.0 # NOTE: if `atom_types_top_p` is `null`, this will be ignored
  atom_types_top_p: null # NOTE: if `null`, argmax-decoding will instead be applied
  data_dir: ${paths.data_dir}
  visualize: true
  save_dir: ${paths.viz_dir}
  removeHs: false # only retain heavy atoms (for molecules)

conditioning:
  dataset_idx: true # always true
  spacegroup: false

datasets: ${data.datamodule.datasets}

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0001
  weight_decay: 0.0

scheduler:
  # _target_: ${resolve_variable:torch.optim.lr_scheduler.LambdaLR}
  # _partial_: true
  # lr_lambda: ${resolve_lr_scheduler:cosine_schedule_with_warmup,10000,1000000,0.5,1e-5}

scheduler_frequency: ${trainer.check_val_every_n_epoch}

# compile model for faster training with pytorch 2.0
compile: true

# using WandB, log model gradients every N steps
log_grads_every_n_steps: 1000
