# @package _global_

# Options for training:
# - /model: [zatom, zatom2]
# - /model/architecture:
#         [mft_80M, mft_180M, mft_500M,
#          met_80M, met_180M, met_500M,
#          mfp_80M, mfp_180M, mfp_500M,
#          mft2_80M, mft2_180M, mft2_500M,
#          tft_5M, tft_20M, tft_70M]
# - /logger: [null, wandb]
# - /trainer: [default, mps, ddp, fsdp]

defaults:
  - override /data: joint
  - override /model: zatom2
  - override /model/architecture: tft_20M
  - override /callbacks: default
  - override /logger: wandb
  - override /strategy: optimized_ddp
  - override /trainer: ddp

trainer:
  check_val_every_n_epoch: 10
  gradient_clip_val: 0.5
  precision: 32-true

model:
  optimizer:
    lr: 2e-3

  scheduler: null # use no scheduler

# precision for PyTorch's floating-point matrix multiplication
float32_matmul_precision: high # NOTE: must be one of (`medium`, `high`, `highest`)
cuda_matmul_allow_tf32: False # allow TF32 on Ampere GPUs, see https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere
cudnn_allow_tf32: False # allow TF32 on Ampere GPUs, see https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere
