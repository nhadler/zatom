# @package _global_

# Options for finetuning:
# - /model: [zatom]
# - /model/architecture:
#         [tft_80M, tft_160M, tft_300M,
#          tfp_80M, tfp_160M, tfp_300M]
# - /logger: [null, wandb]
# - /trainer: [default, mps, ddp, fsdp]

defaults:
  - override /data: joint
  - override /model: zatom
  - override /model/architecture: tft_80M
  - override /callbacks: default_finetuning
  - override /logger: wandb
  - override /strategy: optimized_ddp
  - override /trainer: ddp

name: "finetune_arch-${hydra:runtime.choices.model/architecture}_${hydra:runtime.choices.data}"
task_name: finetune_fm

data:
  datamodule:
    datasets:
      mp20:
        proportion: 0.0
      qm9:
        proportion: 1.0
        global_property:
          [
            mu,
            alpha,
            homo,
            lumo,
            gap,
            r2,
            zpve,
            Cv,
            U0_atom,
            U_atom,
            H_atom,
            G_atom,
          ]
      qmof150:
        proportion: 0.0
      omol25:
        proportion: 0.0
        global_energy: null
      geom:
        proportion: 0.0
      mptrj:
        proportion: 0.0
        global_energy: null
      matbench:
        proportion: 0.0
        global_property: matbench_mp_gap

model:
  architecture:
    num_aux_layers: 4
  augmentations:
    frac_coords: false
  sampling:
    n_jobs: 1
    num_samples: 1
    batch_size: 1

trainer:
  max_epochs: 1000
  max_time: "10:00:00:00"
