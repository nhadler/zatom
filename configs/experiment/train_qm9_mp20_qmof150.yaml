# @package _global_

# Experiment config for QM9+MP20+QMOF150 TFT-80M training
# Multi-dataset setup reproducing ADiT Appendix C

defaults:
  - override /data: joint
  - override /model: zatom
  - override /model/architecture: tft_80M
  - override /callbacks: default
  - override /logger: wandb  # Override to null for smoke tests: logger=null
  - override /strategy: optimized_ddp
  - override /trainer: ddp

name: "train_qm9_mp20_qmof150_arch-${hydra:runtime.choices.model/architecture}"
task_name: train_fm

data:
  datamodule:
    datasets:
      mp20:
        proportion: 1.0
      qm9:
        proportion: 1.0
        global_property: null  # Generation task, not property prediction
      qmof150:
        proportion: 1.0
      omol25:
        proportion: 0.0
        global_energy: null
      geom:
        proportion: 0.0
      mptrj:
        proportion: 0.0
        global_energy: null
      matbench:
        proportion: 0.0
        global_property: matbench_mp_gap
    batch_size:
      train: 64  # Per-GPU batch size (adjust if OOM)
      val: 64
      test: 64
      base_world_size: 2
      base_accumulate_grad_batches: 1

# Training hyperparameters aligned with ADiT Appendix C
trainer:
  max_epochs: 10000
  max_time: "48:00:00:00"  # 48 hours max
  check_val_every_n_epoch: 1
  accumulate_grad_batches: 1  # Gradient accumulation (increase if needed for larger effective batch size)
  devices: 2
  num_nodes: 1

# Model configuration
model:
  architecture:
    num_aux_layers: 0  # No auxiliary task layers for generation-only training
  augmentations:
    frac_coords: true
    pos: true
    multiplicity: 4  # Data augmentation multiplicity
  sampling:
    num_samples: 1000  # Number of samples for validation
    batch_size: 100
    steps: 100  # ODE steps for sampling
    n_jobs: 16
    cfg_scale: 0.0  # No classifier-free guidance
    visualize: false  # Disable visualization generation to save time


