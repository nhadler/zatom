# @package _global_

# Options for training:
# - /model: [zatom, zatom2]
# - /model/architecture:
#         [mft_80M, mft_180M, mft_500M,
#          met_80M, met_180M, met_500M,
#          mfp_80M, mfp_180M, mfp_500M,
#          mft2_80M, mft2_180M, mft2_500M,
#          tft_5M, tft_20M, tft_70M]
# - /logger: [null, wandb]
# - /trainer: [default, mps, ddp, fsdp]

defaults:
  - override /data: joint
  - override /model: zatom2
  - override /model/architecture: tft_70M
  - override /callbacks: default
  - override /logger: wandb
  - override /trainer: ddp

# All parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

name: "overfit_arch-${hydra:runtime.choices.model/architecture}_${hydra:runtime.choices.data}_molecule-and-material"
seed: 42
task_name: overfit_fm

trainer:
  check_val_every_n_epoch: 500
  gradient_clip_val: 0.5
  max_epochs: 20000
  overfit_batches: 1
  precision: 32-true

data:
  datamodule:
    batch_size:
      train: 2
      val: 2
      test: 2
    num_workers:
      train: 0
      val: 0
      test: 0
    datasets:
      mp20:
        proportion: 4e-5
      qm9:
        proportion: 1e-5
      qmof150:
        proportion: 0.0
      omol25:
        proportion: 0.0
      geom:
        proportion: 0.0

model:
  log_grads_every_n_steps: 100

  optimizer:
    lr: 1e-4

  sampling:
    num_samples: 10
    batch_size: 10

  scheduler: null # use no scheduler

# precision for PyTorch's floating-point matrix multiplication
float32_matmul_precision: high # NOTE: must be one of (`medium`, `high`, `highest`)
cuda_matmul_allow_tf32: False # allow TF32 on Ampere GPUs, see https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere
cudnn_allow_tf32: False # allow TF32 on Ampere GPUs, see https://pytorch.org/docs/stable/notes/cuda.html#tf32-on-ampere
