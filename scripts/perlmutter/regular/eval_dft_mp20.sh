#!/bin/bash -l

# Determine location of the project's directory
# PROJECT_ID="dasrepo"
# PROJECT_DIR="/global/cfs/cdirs/$PROJECT_ID/$USER/Repositories/zatom"            # long term storage community drive
PROJECT_DIR="/pscratch/sd/${USER:0:1}/$USER/Repositories/zatom"                   # high-performance storage scratch drive with an 8-week purge policy
cd "$PROJECT_DIR" || exit

# Establish environment variables
# export TORCH_HOME="/global/cfs/cdirs/$PROJECT_ID/$USER/torch_cache"             # long term storage community drive
# export HF_HOME="/global/cfs/cdirs/$PROJECT_ID/$USER/hf_cache"                   # long term storage community drive
export TORCH_HOME="/pscratch/sd/${USER:0:1}/$USER/torch_cache"                    # high-performance storage scratch drive with an 8-week purge policy
export HF_HOME="/pscratch/sd/${USER:0:1}/$USER/hf_cache"                          # high-performance storage scratch drive with an 8-week purge policy

mkdir -p "$TORCH_HOME"
mkdir -p "$HF_HOME"

# Define run details
export PROJECT_ROOT="$PROJECT_DIR/forks/flowmm"
export PMG_VASP_PSP_DIR="$PROJECT_ROOT/data/vasp_psp"

# Change as needed
# mp20_only_zatom_1_eval_dir="$PROJECT_DIR/logs/eval_fm/runs/eval_tft_80M_MP20-only_hxip8xly_2025-12-21_17-00-00"
# jointly_trained_platom_1_eval_dir="$PROJECT_DIR/logs/eval_fm/runs/eval_tfp_80M_QM9+MP20_odrl69x1_2026-01-01_16-30-00"
eval_dir="$PROJECT_DIR/logs/eval_fm/runs/eval_tft_80M_MP20_izr5qhhf_2025-12-16_20-00-00"
# jointly_trained_zatom_1_l_eval_dir="$PROJECT_DIR/logs/eval_fm/runs/eval_tft_160M_QM9+MP20_zkiysa4a_2025-12-24_06-00-00"
# jointly_trained_zatom_1_xl_eval_dir="$PROJECT_DIR/logs/eval_fm/runs/eval_tft_300M_QM9+MP20_i1upnjuo_2025-12-24_06-00-00"

eval_for_dft_samples="$eval_dir/mp20_test_0"
eval_for_dft_json="$eval_dir/mp20_test_0.json"
eval_log_dir="$eval_dir/chgnet_log_dir"

num_jobs=1
slurm_qos=shared
slurm_account=m5008
slurm_partition=nersc
slurm_additional_parameters='{"constraint": "gpu&hbm40g", "module": "gpu,nccl-plugin"}'

# Consolidate
eval_for_dft_pt=$(python "$PROJECT_DIR/forks/flowmm/scripts_model/evaluate.py" consolidate "$eval_for_dft_samples" --subdir "mp20_test_0" --path_eval_pt eval_for_dft.pt | tail -n 1)

# Pre-relax
bash -c "
    unset NCCL_CROSS_NIC && \
    TORCH_HOME=$TORCH_HOME HF_HOME=$HF_HOME \
    srun --kill-on-bad-exit=1 python $PROJECT_DIR/forks/flowmm/scripts_analysis/prerelax.py $eval_for_dft_pt $eval_for_dft_json $eval_log_dir \
    --num_jobs $num_jobs \
    --slurm_qos $slurm_qos \
    --slurm_account $slurm_account \
    --slurm_additional_parameters $slurm_additional_parameters \
    --slurm_partition $slurm_partition
"

# # DFT
# dft_dir="$eval_dir/dft"
# mkdir -p "$dft_dir"
# python "$PROJECT_DIR/forks/flowmm/scripts_analysis/dft_create_inputs.py" "$eval_for_dft_json" "$dft_dir"

# # Energy above hull
# json_e_above_hull="$eval_dir/ehulls.json"
# python "$PROJECT_DIR/forks/flowmm/scripts_analysis/ehull.py" "$eval_for_dft_json" "$json_e_above_hull"

# # Corrected energy above hull
# root_dft_clean_outputs="$eval_dir"
# ehulls_corrected_json="$eval_dir/ehulls_corrected.json"
# python "$PROJECT_DIR/forks/flowmm/scripts_analysis/ehull_correction.py" "$eval_for_dft_json" "$ehulls_corrected_json" --root_dft_clean_outputs "$root_dft_clean_outputs"

# # S.U.N.
# sun_json=sun.json
# python "$PROJECT_DIR/forks/flowmm/scripts_analysis/novelty.py" "$eval_for_dft_json" "$sun_json" --ehulls "$ehulls_corrected_json"
