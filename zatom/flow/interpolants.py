"""Flow matching interpolants.

Adapted from:
    - https://github.com/carlosinator/tabasco
    - https://github.com/jasonkyuyim/multiflow
    - https://github.com/facebookresearch/all-atom-diffusion-transformer
"""

from abc import ABC, abstractmethod
from typing import Callable, Literal, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from tensordict import TensorDict
from torch import Tensor

from zatom.flow.path import FlowPath
from zatom.utils.metric_utils import split_losses_by_time
from zatom.utils.tensor_utils import apply_mask, mask_and_zero_com
from zatom.utils.typing_utils import typecheck


class Interpolant(ABC):
    """Abstract base class for data-noise interpolation.

    Subclasses must implement four domain-specific operations:
    1. sample_noise():    Draw a noise tensor matching the data layout.
    2. create_path():     Build the interpolation path between two data points for a given time t.
    3. compute_loss():    Return a supervised loss for a model prediction along the path.
    4. step():            Advance the system one explicit-Euler step during sampling.

    All methods work on batched `TensorDict` objects; the data entry is accessed via
    `key` and its padding mask via `key_pad_mask`.

    Args:
        key: Key to the data object of interest in the passed batch TensorDict.
        key_pad_mask: Key to the padding mask in the batch TensorDict.
        loss_weight: Scalar weight applied to the computed loss.
        path_t_min: Minimum time value for the interpolation path. Default is 0.0.
        path_t_max: Maximum time value for the interpolation path. Default is 1.0.
        time_factor: Optional callable `f(t)` that rescales the per-sample loss as a
            function of the interpolation time `t`.
        sample_schedule: Optional schedule for sampling times during training.
            One of (`linear`, `power`, `log`).
    """

    @typecheck
    def __init__(
        self,
        key: str,
        key_pad_mask: str = "padding_mask",
        loss_weight: float = 1.0,
        path_t_min: float = 0.0,
        path_t_max: float = 1.0,
        time_factor: Callable | None = None,
        sample_schedule: Literal["linear", "power", "log"] = "linear",
    ):
        self.key = key
        self.key_pad_mask = key_pad_mask
        self.loss_weight = loss_weight
        self.path_t_min = path_t_min
        self.path_t_max = path_t_max
        self.time_factor = time_factor
        self.sample_schedule = sample_schedule

    @typecheck
    @abstractmethod
    def sample_noise(self, shape: torch.Size, pad_mask: Tensor) -> Tensor:
        """Draw a random noise tensor compatible with the data layout.

        Args:
            shape: Desired tensor shape, usually `batch[self.key].shape`.
            pad_mask: Boolean/int mask where 1 indicates padded positions; noise must be
                zeroed at these indices.

        Returns:
            Noise tensor of shape `shape` located on the same device as `pad_mask`.
        """
        pass

    @typecheck
    @abstractmethod
    def create_path(
        self, x_1: TensorDict, t: Tensor, x_0: TensorDict | None = None
    ) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
        """Construct the interpolation triple `(x_0, x_t, dx_t, x_1)` for time `t`.

        Args:
            x_1: TensorDict containing the reference data point at *t = 1*.
            t: Tensor of shape `(B,)` with interpolation times in `[0, 1]`.
            x_0: Optional TensorDict with a pre-sampled noise state; if `None` a new
                one is drawn via `sample_noise()`.

        Returns:
            * x_0: Initial noise state.
            * x_t: Interpolated state at time `t`.
            * dx_t: Velocity, typically `x_1 - x_0`.
            * x_1: Ground truth data point at time `t = 1`.
        """
        pass

    @typecheck
    @abstractmethod
    def compute_loss(
        self, path: FlowPath, pred: TensorDict, compute_stats: bool = True
    ) -> Tuple[Tensor, dict]:
        """Return a supervised loss for a model prediction at time `t`.

        Args:
            path: FlowPath object generated by `create_path`.
            pred: TensorDict with model outputs that correspond to `path.x_1[self.key]`.
            compute_stats: If `True`, also compute and return auxiliary metrics.

        Returns:
            Scalar loss and a (possibly empty) statistics dictionary.
        """
        pass

    @typecheck
    @abstractmethod
    def step(self, batch_t: TensorDict, pred: TensorDict, t: TensorDict, dt: TensorDict) -> Tensor:
        """Advance the sample one explicit-Euler step along the reverse process.

        Args:
            batch_t: TensorDict with the current sample at time `t`.
            pred: Model prediction (same layout as `batch_t`) used to compute the
                velocity field.
            t: TensorDict `(B,)` with current times.
            dt: TensorDict of step sizes to advance.

        Returns:
            Updated data tensor corresponding to time `t + dt`.
        """
        pass


class DiscreteInterpolant(Interpolant):
    """Interpolant between two discrete distributions.

    Args:
        **kwargs: Forwarded to `Interpolant.__init__`.
    """

    @typecheck
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.ce_loss = nn.CrossEntropyLoss(reduction="none")

    @typecheck
    def sample_noise(self, shape: torch.Size, pad_mask: Tensor) -> Tensor:
        """Return uniformly random one-hot noise.

        Args:
            shape: Desired output shape `(â€¦, C)` where `C` equals the number of discrete categories.
            pad_mask: Padding mask; rows with 1s are ignored and set to zeros.

        Returns:
            One-hot encoded noise tensor on the same device as `pad_mask`.
        """
        x_0 = torch.randint(0, shape[-1], shape[:-1], device=pad_mask.device)
        x_0 = F.one_hot(x_0, num_classes=shape[-1])
        return x_0

    @typecheck
    def create_path(
        self, x_1: TensorDict, t: Tensor, x_0: TensorDict | None = None
    ) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
        """Create a path for a ground truth point and a time step.

        Args:
            x_1: TensorDict containing the reference data point at *t = 1*.
            t: Tensor of shape `(B,)` with interpolation times in `[0, 1]`.
            x_0: Optional TensorDict with a pre-sampled noise state; if `None` a new
                one is drawn via `sample_noise()`.

        Returns:
            * x_0: Initial noise state.
            * x_t: Interpolated state at time `t`.
            * dx_t: Velocity, typically `x_1 - x_0`.
            * x_1: Ground truth data point at time `t = 1`.
        """
        if x_0 is None:
            x_0_tensor = self.sample_noise(x_1[self.key].shape, x_1[self.key_pad_mask])
        else:
            x_0_tensor = x_0[self.key]

        t = t.unsqueeze(-1)
        assert t.shape == (
            x_1[self.key].shape[0],
            1,
        ), f"t shape: {t.shape} != {(x_1[self.key].shape[0], 1)}"

        t = t.clamp(self.path_t_min, self.path_t_max)

        _corruption_prob = torch.rand(x_1[self.key].shape[:-1], device=x_1[self.key].device)
        corrupt_mask = (_corruption_prob > t).unsqueeze(-1).int()

        x_t = x_0_tensor * corrupt_mask + x_1[self.key] * (1 - corrupt_mask)

        dx_t = x_1[self.key] - x_0_tensor

        return x_0_tensor, x_t, dx_t, x_1[self.key]

    @typecheck
    def compute_loss(
        self, path: FlowPath, pred: TensorDict, compute_stats: bool = False, eps: float = 1e-6
    ) -> Tuple[Tensor, dict]:
        """Cross-entropy loss between prediction and ground truth.

        Args:
            path: FlowPath from `create_path` (only `path.x_1` is required).
            pred: Model logits with shape `(B, N, C)`.
            compute_stats: Whether to return an empty stats dict (always empty here).
            eps: Small constant to avoid division by zero.

        Returns:
            Mean loss over molecules and an empty statistics dict.
        """
        real_mask = 1 - path.x_1[self.key_pad_mask].int()
        n_tokens = real_mask.sum(dim=-1)

        loss = self.ce_loss(pred[self.key].transpose(1, 2), path.x_1[self.key].argmax(dim=-1))
        per_mol_loss = (loss * real_mask).sum(dim=-1) / (n_tokens + eps)

        if self.time_factor:
            per_mol_loss = per_mol_loss * self.time_factor(path.t[self.key])

        loss_mask = real_mask.any(-1)
        avg_loss = per_mol_loss.sum() / (loss_mask.sum() + eps)

        total_loss = avg_loss * self.loss_weight

        stats_dict = {}
        return total_loss, stats_dict

    @typecheck
    def step(self, batch_t: TensorDict, pred: TensorDict, t: TensorDict, dt: TensorDict) -> Tensor:
        """Stochastic forward-Euler step for discrete states in continuous time.

        Args:
            batch_t: TensorDict containing one-hot states at time `t`.
            pred: Logits predicting the terminal distribution.
            t: TensorDict `(B,)` with current times.
            dt: TensorDict of step sizes to advance.

        Returns:
            One-hot tensor representing the new discrete state.
        """
        t = t[self.key].unsqueeze(-1).unsqueeze(-1)
        dt = dt[self.key].unsqueeze(-1).unsqueeze(-1)
        assert (
            dt.shape == t.shape == (batch_t[self.key].shape[0], 1, 1)
        ), f"t shape: {t.shape}, dt shape: {dt.shape}, batch_t shape: {batch_t[self.key].shape}"

        x1_probs = torch.nn.functional.softmax(pred[self.key], dim=-1)
        curr_state = batch_t[self.key].argmax(dim=-1)

        step_probs = ((dt / (1 - t)) * x1_probs).clamp(max=1.0)
        step_probs.scatter_(-1, curr_state[:, :, None], 0.0)
        step_probs.scatter_(-1, curr_state[:, :, None], 1.0 - step_probs.sum(dim=-1, keepdim=True))
        step_probs = step_probs.clamp(min=0.0)

        step_probs = step_probs / step_probs.sum(dim=-1, keepdim=True)

        x_next = torch.distributions.Categorical(step_probs).sample()
        x_next = F.one_hot(x_next, num_classes=batch_t[self.key].shape[-1])

        return x_next


class CenteredMetricInterpolant(Interpolant):
    """Linear interpolation between two points in Euclidean space.

    This class teaches the model to predict the endpoint of the path.

    Args:
        centered: If True, subtract center-of-mass so translation is ignored.
        scale_noise_by_log_num_tokens: Scale noise amplitude by `log(N_tokens)`.
        noise_scale: Standard deviation of the sampled Gaussian noise.
        **kwargs: Forwarded to `Interpolant.__init__`.
    """

    @typecheck
    def __init__(
        self,
        centered: bool = True,
        scale_noise_by_log_num_tokens: bool = False,
        noise_scale: float = 1.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.mse_loss = nn.MSELoss(reduction="none")
        self.centered = centered
        self.scale_noise_by_log_num_tokens = scale_noise_by_log_num_tokens
        self.noise_scale = noise_scale

        self.mask_fn = mask_and_zero_com if self.centered else apply_mask

    @typecheck
    def sample_noise(self, shape: torch.Size, pad_mask: Tensor) -> Tensor:
        """Return masked Gaussian noise with optional scaling.

        Args:
            shape: Desired output shape.
            pad_mask: Padding mask.

        Returns:
            Noise tensor.
        """
        x_0 = torch.randn(shape, device=pad_mask.device) * self.noise_scale

        if self.scale_noise_by_log_num_tokens:
            num_tokens = (~pad_mask).sum(dim=-1)
            x_0 = x_0 * torch.log(num_tokens[..., None, None])

        x_0 = self.mask_fn(x_0, pad_mask)

        return x_0

    @typecheck
    def create_path(
        self, x_1: TensorDict, t: Tensor, x_0: TensorDict | None = None
    ) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
        """Generate `(x_0, x_t, dx_t, x_1)` via linear interpolation in Euclidean space.

        Args:
            x_1: TensorDict containing the reference data point at *t = 1*.
            t: Tensor of shape `(B,)` with interpolation times in `[0, 1]`.
            x_0: Optional TensorDict with a pre-sampled noise state; if `None` a new
                one is drawn via `sample_noise()`.

        Returns:
            * x_0: Initial noise state.
            * x_t: Interpolated state at time `t`.
            * dx_t: Velocity, typically `x_1 - x_0`.
            * x_1: Ground truth data point at time `t = 1`.
        """
        if x_0 is None:
            x_0_tensor = self.sample_noise(x_1[self.key].shape, x_1[self.key_pad_mask])
        else:
            x_0_tensor = x_0[self.key]

        t = t.unsqueeze(-1).unsqueeze(-1)
        assert t.shape == (
            x_1[self.key].shape[0],
            1,
            1,
        ), f"t shape: {t.shape} != {(x_1[self.key].shape[0], 1, 1)}"

        t = t.clamp(self.path_t_min, self.path_t_max)

        x_0_tensor = self.mask_fn(x_0_tensor, x_1[self.key_pad_mask])
        x_1_tensor = self.mask_fn(x_1[self.key], x_1[self.key_pad_mask])

        x_t = (1.0 - t) * x_0_tensor + t * x_1_tensor
        dx_t = x_1_tensor - x_0_tensor

        return x_0_tensor, x_t, dx_t, x_1_tensor

    @typecheck
    def compute_loss(
        self,
        path: FlowPath,
        pred: TensorDict,
        compute_stats: bool = True,
        pool_mask: bool = False,
        aux_mask: Tensor | None = None,
        eps: float = 1e-6,
    ) -> Tuple[Tensor, dict]:
        """Mean-squared error on masked continuous modalities with optional time weighting.

        Args:
            path: FlowPath containing the true states.
            pred: TensorDict with predicted states.
            compute_stats: Whether to compute and return statistics.
            pool_mask: If True, any-pool the real (i.e., non-padding) mask over the feature dimension.
            aux_mask: Optional mask tensor to apply to the loss calculation. Should be of shape
                `(batch_size, n_tokens)`.
            eps: Small constant to avoid division by zero.

        Returns:
            Total loss tensor.
        """
        real_mask = 1 - path.x_1[self.key_pad_mask].int()
        real_mask = real_mask.any(dim=-1, keepdim=True).int() if pool_mask else real_mask

        assert (
            aux_mask is None or aux_mask.shape == real_mask.shape
        ), f"aux_mask shape: {aux_mask.shape} != real_mask shape: {real_mask.shape}."

        real_mask = real_mask * aux_mask if aux_mask is not None else real_mask
        n_tokens = real_mask.sum(dim=-1)

        err = (pred[self.key] - path.x_1[self.key]) * real_mask.unsqueeze(-1)
        loss = torch.sum(err**2, dim=(-1, -2)) / (n_tokens * err.shape[-1] + eps)

        if self.time_factor:
            loss = loss * self.time_factor(path.t[self.key])

        if compute_stats:
            binned_losses = split_losses_by_time(path.t[self.key], loss, 5)
            stats_dict = {
                **{f"{self.key}_loss_bin_{i}": loss for i, loss in enumerate(binned_losses)},
            }
        else:
            stats_dict = {}

        loss_mask = real_mask.any(-1)
        avg_loss = loss.sum() / (loss_mask.sum() + eps)

        total_loss = avg_loss * self.loss_weight
        return total_loss, stats_dict

    @typecheck
    def step(self, batch_t: TensorDict, pred: TensorDict, t: TensorDict, dt: TensorDict) -> Tensor:
        """Deterministic forward-Euler step for continuous modalities.

        Args:
            batch_t: TensorDict containing the current sample at time `t`.
            pred: Model prediction (same layout as `batch_t`) used to compute the
                velocity field.
            t: TensorDict `(B,)` with current times.
            dt: TensorDict of step sizes to advance.

        Returns:
            Updated data tensor corresponding to time `t + dt`.
        """
        t = t[self.key].unsqueeze(-1).unsqueeze(-1)
        dt = dt[self.key].unsqueeze(-1).unsqueeze(-1)
        assert (
            dt.shape == t.shape == (batch_t[self.key].shape[0], 1, 1)
        ), f"t shape: {t.shape}, dt shape: {dt.shape}, batch_t shape: {batch_t[self.key].shape}"

        x1_pred = pred[self.key]
        velocity = (x1_pred - batch_t[self.key]) / (1 - t)

        x_new = batch_t[self.key] + velocity * dt
        x_new = self.mask_fn(x_new, batch_t[self.key_pad_mask])

        assert (
            x_new.shape == batch_t[self.key].shape
        ), f"x_new shape: {x_new.shape} != {batch_t[self.key].shape}"

        return x_new


class SDEMetricInterpolant(CenteredMetricInterpolant):
    """CenteredMetricInterpolant with Langevin/SDE-style sampling based on the Proteina paper.

    Args:
        langevin_sampling_schedule: Function that returns the sampling schedule for the score.
        white_noise_sampling_scale: Standard deviation of the sampled white noise.
        **kwargs: Forwarded to `Interpolant.__init__`.
    """

    @typecheck
    def __init__(
        self,
        langevin_sampling_schedule: Callable | None = None,
        white_noise_sampling_scale: float = 1.0,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.mse_loss = nn.MSELoss(reduction="none")
        self.white_noise_sampling_scale = white_noise_sampling_scale

        if langevin_sampling_schedule is None:
            self.langevin_sampling_schedule = lambda t: torch.zeros_like(t)
        else:
            self.langevin_sampling_schedule = langevin_sampling_schedule

    @typecheck
    def calculate_score(self, v_t: Tensor, x_t: Tensor, t: Tensor) -> Tensor:
        """
        Return the diffusion score `(t * v_t - x_t) / (1 - t)` as used in Proteina.

        Args:
            v_t: Velocity at time t.
            x_t: Position at time t.
            t: Time tensor.

        Returns:
            Score tensor.
        """
        return (t * v_t - x_t) / (1 - t + 1e-6)

    @typecheck
    def step(self, batch_t: TensorDict, pred: TensorDict, t: TensorDict, dt: TensorDict) -> Tensor:
        """Forward Euler integration step with score components and white noise injection.

        Args:
            batch_t: TensorDict containing the current sample at time `t`.
            pred: Model prediction (same layout as `batch_t`) used to compute the
                velocity field.
            t: TensorDict `(B,)` with current times.
            dt: TensorDict of step sizes to advance.

        Returns:
            Updated data tensor corresponding to time `t + dt`.
        """
        t = t[self.key].unsqueeze(-1).unsqueeze(-1)
        dt = dt[self.key].unsqueeze(-1).unsqueeze(-1)
        assert (
            dt.shape == t.shape == (batch_t[self.key].shape[0], 1, 1)
        ), f"t shape: {t.shape}, dt shape: {dt.shape}, batch_t shape: {batch_t[self.key].shape}"

        x1_pred = pred[self.key]
        velocity = (x1_pred - batch_t[self.key]) / (1 - t)

        score = self.calculate_score(velocity, batch_t[self.key], t)

        component_score = self.langevin_sampling_schedule(t) * score

        wiener_noise_scale = torch.sqrt(
            2 * self.langevin_sampling_schedule(t) * self.white_noise_sampling_scale
        ) * torch.randn_like(batch_t[self.key])
        white_noise = (
            self.sample_noise(batch_t[self.key].shape, batch_t[self.key_pad_mask])
            * wiener_noise_scale
        )

        x_new = batch_t[self.key] + velocity * dt + component_score * dt + white_noise * dt
        x_new = self.mask_fn(x_new, batch_t[self.key_pad_mask])

        # assert (
        #     x_new.shape == batch_t[self.key].shape
        # ), f"x_new shape: {x_new.shape} != {batch_t[self.key].shape}"

        return x_new
